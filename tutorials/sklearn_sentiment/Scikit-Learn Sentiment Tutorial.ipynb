{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Sentiment Bot\n",
    "\n",
    "This is a binary text classification problem, that is sentiment is either negative or positive.\n",
    "\n",
    "In this tutorial, we will create a sentiment model by\n",
    "\n",
    "1.  Create a featurizer to extract important features from a movie review database.\n",
    "2.  Classify the text features with a linear classifier along with performing a grid search to tune the hyperparameters.\n",
    "\n",
    "The author of the code to create this model was written by Olivier Grisel <olivier.grisel@ensta.org> under the Simplified BSD license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you will\n",
    "\n",
    "1.  [Train the model](#train)\n",
    "\n",
    "2.  [Explore the scikit-learn connector](#explore)\n",
    "\n",
    "3.  [Explore the sentiment bot code](#bot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# For when background is dark:\n",
    "sns.set(rc={'text.color': 'white', 'axes.labelcolor': 'white'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train'></a>\n",
    "# Train the model\n",
    "Navigate to `tutorials/sentiment/` on the command line to begin working with the files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "Go into the `movie_reviews` folder and run the `fetch_data.py` command as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! cd movie_reviews; python fetch_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mneg\u001b[m\u001b[m \u001b[34mpos\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls movie_reviews/txt_sentoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the negative and positive movie review files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "# the training data folder must be passed as first argument\n",
    "movie_reviews_data_folder = 'movie_reviews/txt_sentoken'\n",
    "# load_files loads text files where the subfolders are the labels\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset in training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text features and train model\n",
    "\n",
    "![https://imgs.xkcd.com/comics/malamanteau.png](https://imgs.xkcd.com/comics/malamanteau.png)\n",
    "\n",
    "We will use `sklearn`'s `CountVectorizer` to extract features from the text documents as n-gram tokens.  This creates a matrix across all documents, recording counts for the tokens.\n",
    "\n",
    "What is an n-gram?  Take a look at the formation of unigrams, bigrams and trigrams below.\n",
    "\n",
    "![n-gram](https://www.simplicity.be/articles/throwing-dices-recognizing-west-flemish-and-other-languages/img/ngrams.jpg)\n",
    "\n",
    "\n",
    "We also will use term frequency inverse document frequency (TFIDF), in the form of `TfidfTransformer`, on the n-gram tokens to make very common words contribute less information and boost the significance of less common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to see **Piplelines.**  A pipeline is used in `sklearn` to link steps together to automate the training process, testing process etc. and also make hyperparameter tuning across a set of steps easier in combination with a grid search.\n",
    "\n",
    "So, we'll build a featurizer / classifier pipeline that filters out tokens that are too rare or too frequent (using TFIDF and applying a linear classifier).\n",
    "\n",
    "We'll use a linear support vector machine (SVM) classifier, which is widely regarded as one of the best text classification algorithms (although itâ€™s a bit slow to train).\n",
    "\n",
    "If you're wondering what an SVM classifier is, think of it this way:  there are two hyperplanes that can form a gap that separates two classes of data points.  That gap or margin is then called the _maximum margin_ and has a certain width (shown here in 2D space).  Certain data points (which can be N-dimensional vectors) that lie on the one of the two hyperplanes are called _support vectors_.  The plane that is half-way between these two marginal hyperplanes is called the _maximum-margin hyperplane_.\n",
    "\n",
    "\n",
    "\n",
    "![SVM wikipedia](http://www.saedsayad.com/images/SVM_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following just initializes the Pipeline - we haven't trained anything yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a grid search to find out whether unigrams or bigrams are more useful.\n",
    "\n",
    "Fit the pipeline on the training set using grid search for the parameters.  Grid search is how we do our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'clf__C': (10, 100, 1000), 'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__C': (10, 100, 1000),\n",
    "}\n",
    "\n",
    "# The multiprocessing module is used as the backend of joblib.Parallel \n",
    "# when n_jobs != 1\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the score and parameters that landed us the best classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.839333333333\n",
      "clf__C: 10\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Score: ', grid_search.best_score_)                              \n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, grid_search.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the outcome on the testing set and store it in a variable named y_predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = grid_search.predict(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.81      0.82       257\n",
      "        pos       0.81      0.82      0.81       243\n",
      "\n",
      "avg / total       0.82      0.82      0.82       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAFJCAYAAAAWit+oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c66f5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "df = pd.DataFrame(cm)\n",
    "df.columns = ['neg', 'pos']\n",
    "df.index = ['neg', 'pos']\n",
    "cm_heatmap = sns.heatmap(df, \n",
    "            square=True, \n",
    "            annot=True, \n",
    "            cbar=False, \n",
    "            cmap=\"YlGnBu\", \n",
    "            xticklabels=True,\n",
    "            yticklabels=True)\n",
    "\n",
    "\n",
    "# if you don't have seaborn just comment out the sns line and uncomment:\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist our model by saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid_search, 'sentiment.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='explore'></a>\n",
    "# Explore the `scikit-learn` connector\n",
    "\n",
    "Test the model with the scikit-learn connector (sklearn_connector module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model from Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the sentiment classifier by clicking [https://odsc2017.blob.core.windows.net/models/sentiment.pkl](https://odsc2017.blob.core.windows.net/models/sentiment.pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `pybotframework`'s scikit-learn connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybotframework.sklearn_connector import SklearnConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_names = ['neg', 'pos'] # order does matter\n",
    "sklearn_lang_conn = SklearnConnector(model_file='sentiment.pkl',\n",
    "                                     target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_lang_conn.respond(\"I really liked it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_lang_conn.respond(\"I thought it was hilarious.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_lang_conn.respond(\"I thought it could have used better music.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_lang_conn.respond(\"The singing was atrocious.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the bot will use to understand the sentiment of what you write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bot'></a>\n",
    "\n",
    "# Sentiment Bot Example\n",
    "Now we will go over the Lang Bot code at `examples/lang_bot` to see how this bot is made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Download [this](https://odsc2017.blob.core.windows.net/models/text_clf.zip) language model.  Instatiate a connector with this model file and the following class labels from the 20 newsgroups dataset, then test the connector.\n",
    "Labels:\n",
    "```\n",
    "'alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc'\n",
    "```\n",
    "3.  See if you can use the RegexConnector from the first tutorial, add this connector to your bot and then chain them together when you instantiate your BotFramework object.\n",
    "```\n",
    "   BotFramework(connectors=[regex_conn, sklearn_conn], ...)\n",
    "```\n",
    "2.  Update the classifier to a RandomForestClassifier and tune on the n_estimators with grid search.\n",
    "3.  Find a more complex, larger corpus and extract features to train a new model (such as [this](http://ai.stanford.edu/~amaas/data/sentiment/) Stanford one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
