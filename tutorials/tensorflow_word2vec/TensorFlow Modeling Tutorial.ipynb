{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Modeling Tutorial\n",
    "In this tutorial you will learn how to create a word2vec model. TensorFlow provides a detailed word2vec tutorial [here](https://www.tensorflow.org/tutorials/word2vec). In this Notebook, I will just highlight the main features of the model and give a general introduction to word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Word2Vec is a model that was created by [Mikolov et al.](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). It uses the concept of \"word embeddings\", which is a way to represent relationships between words using vectors. For example, let's say you are given the words 'king' and 'queen', there exists a vector that can connect 'king' to 'queen'. Now, take the word 'man. If the vector is projected from 'man', the word2vec model would point to the word 'woman'. This means that this model can use learned embeddings to predict relationships between words it hasn't seen.\n",
    "\n",
    "Here is an example of an embedding matrix taken from the TensorFlow tutorial:\n",
    "\n",
    "![embedding_matrix](https://www.tensorflow.org/images/tsne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "One must be careful when creating an embedding matrix that gender bias is not included. There is a good article [here](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pd)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `word2vec_optimized.py`\n",
    "The TensorFlow team created a word2vec model, word2vec_optimized.py, which I have included in the pybotframework repo to make it easier to run. Let's take a look at the main parts of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ops file\n",
    "TensorFlow uses ops files to stores common operations performed by this package. In our case, we need to load an ops file that is not included with the standard TensorFlow installation. We are going to extend TensorFlow by using the **word2vec_ops.so** ops file. Please see the install instructions in the **examples/tf_bot/** directory if you have not already installed the word2vec ops file on your computer. In **word2vec_optimized.py**, the ops file is loaded using the following command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word2vec = tf.load_op_library(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'word2vec_ops.so'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flags\n",
    "Various parameters in a TensorFlow session can be defined using flags. For example, hyperparameters can be stored using flags. In word2vec_optimized.py, the following flags are used:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\"save_path\", None, \"Directory to write the model.\")\n",
    "flags.DEFINE_string(\n",
    "    \"train_data\", None,\n",
    "    \"Training data. E.g., unzipped file http://mattmahoney.net/dc/text8.zip.\")\n",
    "flags.DEFINE_string(\n",
    "    \"eval_data\", None, \"Analogy questions. \"\n",
    "    \"See README.md for how to get 'questions-words.txt'.\")\n",
    "flags.DEFINE_integer(\"embedding_size\", 200, \"The embedding dimension size.\")\n",
    "flags.DEFINE_integer(\n",
    "    \"epochs_to_train\", 15,\n",
    "    \"Number of epochs to train. Each epoch processes the training data once \"\n",
    "    \"completely.\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.025, \"Initial learning rate.\")\n",
    "flags.DEFINE_integer(\"num_neg_samples\", 25,\n",
    "                     \"Negative samples per training example.\")\n",
    "flags.DEFINE_integer(\"batch_size\", 500,\n",
    "                     \"Numbers of training examples each step processes \"\n",
    "                     \"(no minibatching).\")\n",
    "flags.DEFINE_integer(\"concurrent_steps\", 12,\n",
    "                     \"The number of concurrent training steps.\")\n",
    "flags.DEFINE_integer(\"window_size\", 5,\n",
    "                     \"The number of words to predict to the left and right \"\n",
    "                     \"of the target word.\")\n",
    "flags.DEFINE_integer(\"min_count\", 5,\n",
    "                     \"The minimum number of word occurrences for it to be \"\n",
    "                     \"included in the vocabulary.\")\n",
    "flags.DEFINE_float(\"subsample\", 1e-3,\n",
    "                   \"Subsample threshold for word occurrence. Words that appear \"\n",
    "                   \"with higher frequency will be randomly down-sampled. Set \"\n",
    "                   \"to 0 to disable.\")\n",
    "flags.DEFINE_boolean(\n",
    "    \"interactive\", False,\n",
    "    \"If true, enters an IPython interactive session to play with the trained \"\n",
    "    \"model. E.g., try model.analogy(b'france', b'paris', b'russia') and \"\n",
    "    \"model.nearby([b'proton', b'elephant', b'maxwell'])\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Options()`\n",
    "An options class is used to retrieve flag values and pass them to the algorithem.\n",
    "\n",
    "\n",
    "## `Word2Vec()`\n",
    "This class defines the word2vec object. This is where all of the modeling is done. Let's take a look at a few of the methods.\n",
    "\n",
    "### `read_analogies()`\n",
    "Read in a list of word analogies. Each line of the text file consists of four words; the first two words are an analogy of the last two words. The model will use this file during training.\n",
    "\n",
    "### `build_graph()`\n",
    "Build the TensorFlow graph. \n",
    "\n",
    "### `build_eval_graph()`\n",
    "Build the evaluation graph. Batches of three words will be analyzed. The embedding vector for each word will be collected. This will be used to predict the embedding vector for the fourth, unknown word.\n",
    "\n",
    "### `train()`\n",
    "Set up a TensorFlow session and train the model.\n",
    "\n",
    "### `eval()`\n",
    "Evaluate the model accuracy and report it.\n",
    "\n",
    "### `analogy()`\n",
    "This function is used with the final, trained model. The user supplies three words and the model tries to predict the fourth word.\n",
    "\n",
    "### `nearby()`\n",
    "This function is also used with the final, trained model. Print out nearby words for a list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling in Practice\n",
    "Now that we have a brief understanding of how word2vec works, let's train a model. This section will follow the script, tf_train_model.py in the _examples/tf_bot/_ directory. First, we need to import some things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import examples.tf_bot.word2vec.word2vec_optimized as word2vec_optimized\n",
    "from examples.tf_bot.word2vec.word2vec_optimized import Options, Word2Vec, FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to read in the TensorFlow ops file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some TensorFlow flags are originally set to `None` as place holders. We need to define these so that the model can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repo_path = '/'.join(os.getcwd().split('/')[0:-2])\n",
    "\n",
    "# Save model to the current path\n",
    "FLAGS.save_path = os.path.join(repo_path, 'examples/tf_bot/data/')\n",
    "\n",
    "# Dataset to train the model\n",
    "FLAGS.train_data = os.path.join(repo_path, 'examples/tf_bot/data/text8_trimmed.txt')\n",
    "\n",
    "# A list of analogies to test the model\n",
    "FLAGS.eval_data = os.path.join(repo_path, 'examples/tf_bot/data/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: repo_path will depend on where you cloned the repo on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can setup our TensorFlow graph and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file:  /Users/dave/DataScience/Projects/GitHub/pybotframework/examples/tf_bot/data/text8_trimmed.txt\n",
      "Vocab size:  1467  + UNK\n",
      "Words per epoch:  49999\n",
      "Eval analogy file:  /Users/dave/DataScience/Projects/GitHub/pybotframework/examples/tf_bot/data/questions-words.txt\n",
      "Questions:  76\n",
      "Skipped:  19468\n",
      "Epoch    1 Step      327: lr = 0.024 words/sec =     5700\n",
      "Eval    0/76 accuracy =  0.0%\n",
      "Epoch    2 Step      661: lr = 0.023 words/sec =     5812\n",
      "Eval    0/76 accuracy =  0.0%\n",
      "Epoch    3 Step      995: lr = 0.022 words/sec =     5820\n",
      "Eval    1/76 accuracy =  1.3%\n",
      "Epoch    4 Step     1329: lr = 0.021 words/sec =     5859\n",
      "Eval    1/76 accuracy =  1.3%\n",
      "Epoch    5 Step     1645: lr = 0.020 words/sec =     5511\n",
      "Eval    1/76 accuracy =  1.3%\n",
      "Epoch    6 Step     1979: lr = 0.019 words/sec =     5819\n",
      "Eval    2/76 accuracy =  2.6%\n",
      "Epoch    7 Step     2313: lr = 0.018 words/sec =     5859\n",
      "Eval    2/76 accuracy =  2.6%\n",
      "Epoch    8 Step     2641: lr = 0.017 words/sec =     5713\n",
      "Eval    2/76 accuracy =  2.6%\n",
      "Epoch    9 Step     2963: lr = 0.016 words/sec =     5619\n",
      "Eval    2/76 accuracy =  2.6%\n",
      "Epoch   10 Step     3297: lr = 0.015 words/sec =     5835\n",
      "Eval    2/76 accuracy =  2.6%\n",
      "Epoch   11 Step     3630: lr = 0.014 words/sec =     5777\n",
      "Eval    1/76 accuracy =  1.3%\n",
      "Epoch   12 Step     3964: lr = 0.013 words/sec =     5810\n",
      "Eval    3/76 accuracy =  3.9%\n",
      "Epoch   13 Step     4286: lr = 0.013 words/sec =     5610\n",
      "Eval    2/76 accuracy =  2.6%\n",
      "Epoch   14 Step     4620: lr = 0.012 words/sec =     5815\n",
      "Eval    1/76 accuracy =  1.3%\n",
      "Epoch   15 Step     4954: lr = 0.011 words/sec =     5813\n",
      "Eval    2/76 accuracy =  2.6%\n"
     ]
    }
   ],
   "source": [
    "opts = Options()\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        model = Word2Vec(opts, session)\n",
    "        model.read_analogies()\n",
    "    for _ in range(opts.epochs_to_train):\n",
    "        model.train()\n",
    "        model.eval()\n",
    "    # Save the model after training has finished\n",
    "    model.saver.save(session, os.path.join(opts.save_path, \"model.ckpt\"),\n",
    "                     global_step=model.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises:\n",
    "\n",
    "1. Using the text8_trimmed.txt file and the questions-words.txt file, create a TensorFlow model and train it.\n",
    "\n",
    "2. Think of three analogies, each analogy consisting of four words. Pass three of the words to the trained, TensorFlow model and return a prediction for the fourth word. Hint: You will need to use the analogy() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Exercises:\n",
    "1. As can be seen, the model training did not perform well. One reason is because the train dataset is too small. This was done to save time during the workshop. At home, or if there is time, download the larger training set from here [provide link] and train the model. Does the model peform better with this larger text file?\n",
    "\n",
    "2. Find a large, text document on-line. Download it, reformat it to look like the above training set, and train the model. Are there any improvements in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
